<!doctype html public "-//W3C//DTD HTML 4.01 Transitional//EN"
        "http://www.w3.org/TR/html4/loose.dtd">

<html>

<head>

<title>Postfix Queue Scheduler</title>

<meta http-equiv="Content-Type" content="text/html; charset=us-ascii">

</head>

<body>

<h1><img src="postfix-logo.jpg" width="203" height="98" ALT="">Postfix Queue Scheduler</h1>

<hr>

<h2> Overview </h2>

<p> The queue manager is by far the most complex part of the Postfix
mail system. It schedules delivery of new mail, retries failed
deliveries at specific times, and removes mail from the queue after
the last delivery attempt. Once started, the qmgr(8) process runs
until "postfix reload" or "postfix stop". </p>

<p> As a persistent process, the queue manager has to meet strict
requirements with respect to code correctness and robustness. Unlike
non-persistent daemon processes, the queue manager cannot benefit
from Postfix's process rejuvenation mechanism that limit the impact
from resource leaks and other coding errors. </p>

<p> There are two major classes of mechanisms that control the
operation of the queue manager: </p>

<ul>

<li> <p> Mechanisms concerned with the number of concurrent deliveries
to a specific destination, including decisions on when to suspend
deliveries after persistent failures. These are described under "<a
href="#concurrency">Concurrency scheduling</a>". </p>

<li> <p> Mechanisms concerned with the selection of what mail to
deliver to a given destination.  These are described under "<a
href="#jobs">Preemptive scheduling</a>". </p>

</ul>

<h2> <a name="concurrency"> Concurrency scheduling </a> </h2>

<p> This section documents the Postfix 2.5 concurrency scheduler.
Prior Postfix versions used a simple but robust algorithm where the
per-destination delivery concurrency was decremented by 1 after a
delivery suffered connection or handshake failure, and was incremented
by 1 otherwise.  Of course the concurrency was never allowed to
exceed the maximum per-destination concurrency limit.  And when a
destination's concurrency level dropped to zero, the destination
was declared "dead" and delivery was suspended.  </p>

<p> Drawbacks of the old +/-1 feedback concurrency scheduler are:
<p>

<ul>

<li> <p> Overshoot due to exponential delivery concurrency growth
with each pseudo-cohort(*). For example, with the default initial
concurrency of 5, concurrency would proceed over time as (5-10-20).
</p>

<li> <p> Throttling down to zero concurrency after a single
pseudo-cohort(*) failure. This was especially an issue with
low-concurrency channels where a single failure could be sufficient
to mark a destination as "dead", causing the suspension of further
deliveries to the affected destination. </p>

</ul>

<p> (*) A pseudo-cohort is a number of delivery requests equal to
a destination's delivery concurrency. </p>

<p> The revised concurrency scheduler has a highly modular structure.
It uses separate mechanisms for per-destination concurrency control
and for "dead destination" detection.  The concurrency control in
turn is built from two separate mechanisms: it supports less-than-1
feedback to allow for more gradual concurrency adjustments, and it
uses feedback hysteresis to suppress concurrency oscillations.  And
instead of waiting for delivery concurrency to throttle down to
zero, a destination is declared "dead" after a configurable number
of pseudo-cohorts reports connection or handshake failure.  </p>

<h2> Summary of the Postfix 2.5 concurrency feedback algorithm </h2>

<p> We want to increment a destination's delivery concurrency after
some (not necessarily consecutive) number of deliveries without
connection or handshake failure.  This is implemented with positive
feedback g(N) where N is the destination's delivery concurrency.
With g(N)=1 we get the old scheduler's exponential growth in time,
while g(N)=1/N gives linear growth in time.  Less-than-1 feedback
and integer truncation naturally give us hysteresis, so that
transitions to larger concurrency happen every 1/g(N) positive
feedback events. </p>

<p> We want to decrement a destination's delivery concurrency after
some (not necessarily consecutive) number of deliveries suffer
connection or handshake failure.  This is implemented with negative
feedback f(N) where N is the destination's delivery concurrency.
With f(N)=1 we get the old scheduler's behavior where concurrency
is throttled down dramatically after a single pseudo-cohort failure,
while f(N)=1/N backs off more gently.  Again, less-than-1 feedback
and integer truncation naturally give us hysteresis, so that
transitions to lower concurrency happen every 1/f(N) negative
feedback events. </p>

<p> However, with negative feedback we introduce a subtle twist.
We "reverse" the hysteresis cycle so that the transition to lower
concurrency happens at the <b>beginning</b> of a sequence of 1/f(N)
negative feedback events.  Otherwise, a correction for overload
would be made too late.  In the case of a concurrency-limited server,
this makes the choice of f(N) relatively unimportant, as borne out
by measurements.  </p>

<p> In summary, the main ingredients for the Postfix 2.5 concurrency
feedback algorithm are a) the option of less-than-1 positive feedback
to avoid overwhelming servers, b) the option of less-than-1 negative
feedback to avoid or giving up too fast, c) feedback hysteresis to
avoid rapid oscillation, and c) a "reverse" hysteresis cycle for
negative feedback, so that it can correct for overload quickly. </p>

<h2> Summary of the Postfix 2.5 "dead destination" detection algorithm </h2>

<p> We want to suspend deliveries to a specific destination after
some number of deliveries suffers connection or handshake failure.
The old scheduler declares a destination "dead" when negative (-1)
feedback throttles the delivery concurrency down to zero. With
less-than-1 feedback, this throttling down would obviously take too
long.  We therefore have to separate "dead destination" detection
from concurrency feedback.  This is implemented by introducing the
concept of pseudo-cohort failure. The Postfix 2.5 concurrency
scheduler declares a destination "dead" after a configurable number
of pseudo-cohort failures. The old scheduler corresponds to the
special case where the pseudo-cohort failure limit is equal to 1.
</p>

<h2> Pseudocode for the Postfix 2.5 concurrency scheduler </h2>

<p> The pseudo code shows how the ideas behind new concurrency
scheduler are implemented as of November 2007.  The actual code can
be found in the module qmgr/qmgr_queue.c.  </p>

<pre>
Types:
	Each destination has one set of the following variables
        int window
        double success
        double failure
        double fail_cohorts

Feedback functions:
	N is concurrency; x, y are arbitrary numbers in [0..1] inclusive
        positive feedback: g(N) = x/N | x/sqrt(N) | x
        negative feedback: f(N) = y/N | y/sqrt(N) | y

Initialization:
        window = initial_concurrency
        success = 0
        failure = 0
        fail_cohorts = 0

After success:
        fail_cohorts = 0
        Be prepared for feedback > hysteresis, or rounding error
        success += g(window)
        while (success >= 1)    Hysteresis 1
            window += 1         Hysteresis 1
            failure = 0
            success -= 1        Hysteresis 1
        Be prepared for overshoot
        if (window > concurrency limit)
            window = concurrency limit

Safety:
        Don't apply positive feedback unless
            window < busy_refcount + init_dest_concurrency
        otherwise negative feedback effect could be delayed

After failure:
        if (window > 0)
            fail_cohorts += 1.0 / window
            if (fail_cohorts > cohort_failure_limit)
                window = 0
        if (window > 0)
            Be prepared for feedback > hysteresis, rounding errors
            failure -= f(window)
            while (failure < 0)
                window -= 1     Hysteresis 1
                failure += 1    Hysteresis 1
                success = 0
            Be prepared for overshoot
            if (window < 1)
                window = 1
</pre>

<h2> Results for the Postfix 2.5 concurrency feedback scheduler </h2>

<p> Discussions about the concurrency scheduler redesign started
early 2004, when the primary goal was to find alternatives that did
not exhibit exponential growth or rapid concurrency throttling.  No
code was implemented until late 2007, when the primary concern had
shifted towards better handling of server concurrency limits. For
this reason we measure how well the new scheduler does this
job.  The table below compares mail delivery performance of the old
+/-1 feedback with other feedback functions, for different server
concurrency enforcement methods.  Measurements were done with a
FreeBSD 6.2 client and with FreeBSD 6.2 and various Linux servers.
</p>

<li> Server configuration:

<ul> <li> The mail flow was slowed down with 1 second latency per
recipient ("smtpd_client_restrictions = sleep 1"). The purpose was
to make results less dependent on hardware details, by reducing the
slow-downs by disk I/O, logging I/O, and network I/O.

<li> Concurrency was limited by the server process limit
("default_process_limit = 5", "smtpd_client_event_limit_exceptions
= static:all"). Postfix was stopped and started after changing the
process limit, because the same number is also used as the backlog
argument to the listen(2) system call, and "postfix reload" does
not re-issue this call.

<li> Mail was discarded with "local_recipient_maps = static:all" and
"local_transport = discard". The discard action in header/body checks
could not be used as it fails to update the in_flow_delay counters.

</ul>

<li> Client configuration:

<ul>

<li> Queue file overhead was minimized by sending one message to a
virtual alias that expanded into 2000 different remote recipients.
All recipients were accounted for according to the maillog file.
The virtual_alias_expansion_limit setting was increased to avoid
complaints from the cleanup(8) server.

<li> The number of deliveries was maximized with
"smtp_destination_recipient_limit = 2". A smaller limit would cause
Postfix to schedule the concurrency per recipient instead of domain,
which is not what we want.

<li> Maximal concurrency was limited with
"smtp_destination_concurrency_limit = 20", and
initial_destination_concurrency was set to the same value.

<li> The positive and negative concurrency feedback hysteresis was
1.  Concurrency was incremented by 1 at the END of 1/feedback steps
of positive feedback, and was decremented by 1 at the START of
1/feedback steps of negative feedback.

<li> The SMTP client used the default 30s SMTP connect timeout and
300s SMTP greeting timeout.

</ul>

<p> The first results are for a FreeBSD 6.2 server, where our
artificially low listen(2) backlog results in a very short kernel
queue for established connections. As the table shows, all deferred
deliveries failed due to a 30s connection timeout, and none failed
due to a server greeting timeout.  This measurement simulates what
happens when the server's connection queue is completely full under
load, and the TCP engine drops new connections.  </p>

<blockquote>

<table>

<tr> <th>client<br> limit</th> <th>server<br> limit</th> <th>feedback<br>
style</th> <th>connection<br> caching</th> <th>percentage<br>
deferred</th> <th colspan="2">client concurrency<br> average/stddev</th>
<th colspan=2>timed-out in<br> connect/greeting </th> </tr>

<tr> <td align="center" colspan="9"> <hr> </td> </tr>

<tr><td align="center">20</td> <td align="center">5</td> <td
align="center">1/N</td> <td align="center">no</td> <td
align="center">9.9</td> <td align="center">19.4</td> <td
align="center">0.49</td> <td align="center">198</td> <td
align="center">-</td> </tr>

<tr><td align="center">20</td> <td align="center">5</td> <td
align="center">1/N</td> <td align="center">yes</td> <td
align="center">10.3</td> <td align="center">19.4</td> <td
align="center">0.49</td> <td align="center">206</td> <td
align="center">-</td> </tr>

<tr><td align="center">20</td> <td align="center">5</td> <td
align="center">1/sqrt(N)</td> <td align="center">no</td>
<td align="center">10.4</td> <td align="center">19.6</td> <td
align="center">0.59</td> <td align="center">208</td> <td
align="center">-</td> </tr>

<tr><td align="center">20</td> <td align="center">5</td> <td
align="center">1/sqrt(N)</td> <td align="center">yes</td>
<td align="center">10.6</td> <td align="center">19.6</td> <td
align="center">0.61</td> <td align="center">212</td> <td
align="center">-</td> </tr>

<tr><td align="center">20</td> <td align="center">5</td> <td
align="center">1</td> <td align="center">no</td> <td
align="center">10.1</td> <td align="center">19.5</td> <td
align="center">1.29</td> <td align="center">202</td> <td
align="center">-</td> </tr>

<tr><td align="center">20</td> <td align="center">5</td> <td
align="center">1</td> <td align="center">yes</td> <td
align="center">10.8</td> <td align="center">19.3</td> <td
align="center">1.57</td> <td align="center">216</td> <td
align="center">-</td> </tr>

<tr> <td align="center" colspan="9"> <hr> </td> </tr>

</table>

<p> A busy server with a completely full connection queue.  N is
the client delivery concurrency.  Failed deliveries time out after
30s without completing the TCP handshake. See below for a discussion
of results. </p>

</blockquote>

<p> The next table shows results for a Fedora Core 8 server (results
for RedHat 7.3 are identical). In this case, the listen(2) backlog
argument has little if any effect on the kernel's established
connection queue.  As the table shows, practically all deferred
deliveries fail after the 300s SMTP greeting timeout. As these
timeouts were 10x longer than with the previous measurement, we
increased the recipient count (and thus the running time) by a
factor of 10 to keep the results comparable. </p>

<blockquote>

<table>

<tr> <th>client<br> limit</th> <th>server<br> limit</th> <th>feedback<br>
style</th> <th>connection<br> caching</th> <th>percentage<br>
deferred</th> <th colspan="2">client concurrency<br> average/stddev</th>
<th colspan=2>timed-out in<br> connect/greeting </th> </tr>

<tr> <td align="center" colspan="9"> <hr> </td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/N</td> <td align="center">no</td> <td
align="center">1.16</td> <td align="center">19.8</td> <td
align="center">0.37</td> <td align="center">-</td> <td
align="center">230</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/N</td> <td align="center">yes</td> <td
align="center">1.36</td> <td align="center">19.8</td> <td
align="center">0.36</td> <td align="center">-</td> <td
align="center">272</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/sqrt(N)</td> <td align="center">no</td>
<td align="center">1.21</td> <td align="center">19.9</td> <td
align="center">0.23</td> <td align="center">4</td> <td
align="center">238</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/sqrt(N)</td> <td align="center">yes</td>
<td align="center">1.36</td> <td align="center">20.0</td> <td
align="center">0.23</td> <td align="center">-</td> <td
align="center">272</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1</td> <td align="center">no</td> <td
align="center">1.18</td> <td align="center">20.0</td> <td
align="center">0.16</td> <td align="center">-</td> <td
align="center">236</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1</td> <td align="center">yes</td> <td
align="center">1.39</td> <td align="center">20.0</td> <td
align="center">0.16</td> <td align="center">-</td> <td
align="center">278</td> </tr>

<tr> <td align="center" colspan="9"> <hr> </td> </tr>

</table>

<p> A busy server with a non-full connection queue.  N is the client
delivery concurrency. Failed deliveries complete at the TCP level,
but time out after 300s while waiting for the SMTP greeting.  See
below for a discussion of results.  </p>

</blockquote>


<p> The final concurrency limited result shows what happens when
SMTP connections don't time out, but are rejected immediately with
the Postfix server's smtpd_client_connection_count_limit feature.
Similar results can be expected with concurrency limiting features
built into other MTAs or firewalls.  For this measurement we specified
a server concurrency limit and a client initial destination concurrency
of 5, and a server process limit of 10. The server was FreeBSD 6.2
but that does not matter here, because the "push back" is done
entirely by the server's Postfix itself. </p>

<blockquote>

<table>

<tr> <th>client<br> limit</th> <th>server<br> limit</th> <th>feedback<br>
style</th> <th>connection<br> caching</th> <th>percentage<br>
deferred</th> <th colspan="2">client concurrency<br> average/stddev</th>
<th>theoretical<br>defer rate</th> </tr>

<tr> <td align="center" colspan="9"> <hr> </td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/N</td> <td align="center">no</td> <td
align="center">16.5</td> <td align="center">5.17</td> <td
align="center">0.38</td> <td align="center">1/6</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/N</td> <td align="center">yes</td> <td
align="center">16.5</td> <td align="center">5.17</td> <td
align="center">0.38</td> <td align="center">1/6</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/sqrt(N)</td> <td align="center">no</td>
<td align="center">24.5</td> <td align="center">5.28</td> <td
align="center">0.45</td> <td align="center">1/4</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1/sqrt(N)</td> <td align="center">yes</td>
<td align="center">24.3</td> <td align="center">5.28</td> <td
align="center">0.46</td> <td align="center">1/4</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1</td> <td align="center">no</td> <td
align="center">49.7</td> <td align="center">5.63</td> <td
align="center">0.67</td> <td align="center">1/2</td> </tr>

<tr> <td align="center">20</td> <td align="center">5</td> <td
align="center">1</td> <td align="center">yes</td> <td
align="center">49.7</td> <td align="center">5.68</td> <td
align="center">0.70</td> <td align="center">1/2</td> </tr>

<tr> <td align="center" colspan="9"> <hr> </td> </tr>

</table>

<p> A server with active per-client concurrency limiter that replies
with 421 and disconnects.  N is the client delivery concurrency.
The theoretical mail deferral rate is 1/(1+roundup(1/feedback)).
This is always 1/2 with the fixed +/-1 feedback; with the variable
feedback variants, the defer rate decreases with increasing
concurrency. See below for a discussion of results. </p>

</blockquote>

<p> The results are based on the first delivery runs only; they do
not include any second etc. delivery attempts.  

<p> The first two examples show that the feedback method matters
little when concurrency is limited due to congestion. This is because
the initial concurrency was already at the client's concurrency
maximum, and because there was 10-100 times more positive than
negative feedback.  The contribution from SMTP connection caching
was also minor for these two examples. </p>

<p> In the last example, the old +/-1 feedback scheduler defers 50%
of the mail when confronted with an active (anvil-style) server
concurrency limit, where the server hangs up immediately with a 421
status (a TCP-level RST would have the same result).  Less aggressive
feedback mechanisms fare better here, and the concurrency-dependent
feedback fares even better at higher concurrencies than shown here,
but they have limitations as discussed in the next section.  </p>

<h2> Limitations of less-than-1 feedback </h2>

<p> The delivery concurrency scheduler with less-than-1 feedback
solves a problem with servers that have active concurrency limiters,
but this works well only because feedback is handled in a peculiar
manner: positive feedback increments the concurrency by 1 at the
end of a sequence of events of length 1/feedback, while negative
feedback decrements concurrency by 1 at the beginning of such a
sequence. This is how Postfix adjusts quickly for overshoot without
causing lots of mail to be deferred.  Without this difference in
feedback treatment, less-than-1 feedback would defer 50% of the
mail, and would be no better in this respect than the simple +/-1
feedback scheduler.  </p>

<p> Unfortunately, the same feature that corrects quickly for
concurrency overshoot also makes the scheduler more sensitive for
noisy negative feedback.  The reason is that one lonely negative
feedback event has the same effect as a complete sequence of length
1/feedback: in both cases delivery concurrency is dropped by 1
immediately.  For example, when multiple servers are placed behind
a load balancer on a single IP address, and 1 out of K servers fails
to complete the SMTP handshake, a scheduler with 1/N (N = concurrency)
feedback will stop increasing its concurrency once it reaches roughly
K.  Even though the good servers behind the load balancer are
perfectly capable of handling more mail, the 1/N feedback scheduler
will linger around concurrency K. </p>

<p> This problem with 1/N feedback gets worse as 1/N gets smaller.
A workaround is to use fixed less-than-1 values for positive and
negative feedback that limit the noise sensitivity, for example:
positive feedback of 1/4 and negative feedback 1/10.  Of course
using fixed feedback means concurrency growth is moderated only for
a limited range of concurrencies.  Sites that deliver at per-destination
concurrencies of 50 or more will require special configuration.
</p>

<h2> <a name="jobs"> Preemptive scheduling </a> </h2>

<p> This is the beginning of documentation for a preemptive queue
manager scheduling algorithm by Patrik Rak. For a long time, this
code was made available under the name "nqmgr(8)" (new queue manager),
as an optional module. As of Postfix 2.1 this is the default queue
manager, which is always called "qmgr(8)". The old queue manager
will for some time will be available under the name of "oqmgr(8)".
</p>

<h3>Why the non-preemptive Postfix queue manager was replaced</h3>

<p> The non-preemptive Postfix scheduler had several limitations
due to unfortunate choices in its design. </p>

<ol>

    <li> <p> Round-robin selection by destination for mail that is
    delivered via the same message delivery transport. The round-robin
    strategy was chosen with the intention to prevent a single
    (destination) site from using up too many mail delivery resources.
    However, that strategy penalized inbound mail on bi-directional
    gateways.  The poor suffering inbound destination would be
    selected only 1/number-of-destinations of the time, even when
    it had more mail than other destinations, and thus mail could
    be delayed. </p>

    <p> Victor Duchovni found a workaround: use different message
    delivery transports, and thus avoid the starvation problem.
    The Patrik Rak scheduler solves this problem by using FIFO
    selection. </p>

    <li> <p> A second limitation of the old Postfix scheduler was
    that delivery of bulk mail would block all other deliveries,
    causing large delays.  Patrik Rak's scheduler allows mail with
    fewer recipients to slip past bulk mail in an elegant manner.
    </p>

</ol>

<h3>How the non-preemptive queue manager scheduler works </h3>

<p> The following text is from Patrik Rak and should be read together
with the postconf(5) manual that describes each configuration
parameter in detail. </p>

<p> From user's point of view, oqmgr(8) and qmgr(8) are both the same,
except for how next message is chosen when delivery agent becomes
available.  You already know that oqmgr(8) uses round-robin by destination
while qmgr(8) uses simple FIFO, except for some preemptive magic.
The postconf(5) manual documents all the knobs the user
can use to control this preemptive magic - there is nothing else
to the preemption than the quite simple conditions described in there.
</p>

<p> As for programmer-level documentation, this will have to be
extracted from all those emails we have exchanged with Wietse [rats!
I hoped that Patrik would do the work for me -- Wietse] But I think
there are no missing bits which we have not mentioned in our
conversations. </p>

<p> However, even from programmer's point of view, there is nothing
more to add to the message scheduling idea itself.  There are few
things which make it look more complicated than it is, but the
algorithm is the same as the user perceives it. The summary of the
differences of the programmer's view from the user's view are: </p>

<ol>

    <li> <p> Simplification of terms for users: The user knows
    about messages and recipients. The program itself works with
    jobs (one message is split among several jobs, one per each
    transport needed to deliver the message) and queue entries
    (each entry may group several recipients for same destination).
    Then there is the peer structure introduced by qmgr(8) which is
    simply per-job analog of the queue structure. </p>

    <li> <p> Dealing with concurrency limits: The actual implementation
    is complicated by the fact that the messages (resp. jobs) may
    not be delivered in the exactly scheduled order because of the
    concurrency limits. It is necessary to skip some "blocker" jobs
    when the concurrency limit is reached and get back to them
    again when the limit permits. </p>

    <li> <p> Dealing with resource limits: The actual implementation is
    complicated by the fact that not all recipients may be read in-core.
    Therefore each message has some recipients in-core and some may
    remain on-file. This means that a) the preemptive algorithm needs
    to work with recipient count estimates instead of exact counts, b)
    there is extra code which needs to manipulate the per-transport
    pool of recipients which may be read in-core at the same time, and
    c) there is extra code which needs to be able to read recipients
    into core in batches and which is triggered at appropriate moments. </p>

    <li> <p> Doing things efficiently: All important things I am
    aware of are done in the minimum time possible (either directly
    or at least when amortized complexity is used), but to choose
    which job is the best candidate for preempting the current job
    requires linear search of up to all transport jobs (the worst
    theoretical case - the reality is much better). As this is done
    every time the next queue entry to be delivered is about to be
    chosen, it seemed reasonable to add cache which minimizes the
    overhead. Maintenance of this candidate cache slightly obfuscates
    things.

</ol>

<p> The points 2 and 3 are those which made the implementation
(look) complicated and were the real coding work, but I believe
that to understand the scheduling algorithm itself (which was the
real thinking work) is fairly easy. </p>

</body>

</html>
